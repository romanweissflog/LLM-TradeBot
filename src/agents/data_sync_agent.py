"""
æ•°æ®å…ˆçŸ¥ (The Oracle) Agent

èŒè´£ï¼š
1. å¼‚æ­¥å¹¶å‘è¯·æ±‚å¤šå‘¨æœŸKçº¿æ•°æ®
2. æ‹†åˆ† stable/live åŒè§†å›¾
3. æ—¶é—´å¯¹é½éªŒè¯

ä¼˜åŒ–ç‚¹ï¼š
- å¹¶å‘IOï¼ŒèŠ‚çœ60%æ—¶é—´
- åŒè§†å›¾æ•°æ®ï¼Œè§£å†³æ»åé—®é¢˜
"""

import asyncio
import pandas as pd
from datetime import datetime
from typing import Dict, List, Tuple
from dataclasses import dataclass, field

from src.api.binance_client import BinanceClient
from src.api.quant_client import quant_client
from src.utils.logger import log
from src.utils.oi_tracker import oi_tracker
from src.utils.kline_cache import get_kline_cache


@dataclass
class MarketSnapshot:
    """
    å¸‚åœºå¿«ç…§ï¼ˆåŒè§†å›¾ç»“æ„ï¼‰
    
    stable_view: iloc[:-1] å·²å®Œæˆçš„Kçº¿ï¼Œç”¨äºè®¡ç®—å†å²æŒ‡æ ‡
    live_view: iloc[-1] å½“å‰æœªå®Œæˆçš„Kçº¿ï¼ŒåŒ…å«æœ€æ–°ä»·æ ¼
    """
    # 5m æ•°æ®
    stable_5m: pd.DataFrame  # å·²å®ŒæˆKçº¿
    live_5m: Dict            # æœ€æ–°Kçº¿
    
    # 15m æ•°æ®
    stable_15m: pd.DataFrame
    live_15m: Dict
    
    # 1h æ•°æ®
    stable_1h: pd.DataFrame
    live_1h: Dict
    
    # å…ƒæ•°æ®
    timestamp: datetime
    alignment_ok: bool       # æ—¶é—´å¯¹é½çŠ¶æ€
    fetch_duration: float    # è·å–è€—æ—¶ï¼ˆç§’ï¼‰
    
    # å¯¹å¤–é‡åŒ–æ·±åº¦æ•°æ® (Netflow, OI)
    quant_data: Dict = field(default_factory=dict)
    
    # Binance åŸç”Ÿæ•°æ® (Native Data)
    binance_funding: Dict = field(default_factory=dict)
    binance_oi: Dict = field(default_factory=dict)
    
    # åŸå§‹æ•°æ®ï¼ˆå¯é€‰ï¼Œç”¨äºè°ƒè¯•ï¼‰
    raw_5m: List[Dict] = field(default_factory=list)
    raw_15m: List[Dict] = field(default_factory=list)
    raw_1h: List[Dict] = field(default_factory=list)
    
    # ğŸ”§ FIX: Added symbol for pipeline tracking (must come after fields with defaults)
    symbol: str = "UNKNOWN"


class DataSyncAgent:
    """
    æ•°æ®å…ˆçŸ¥ (The Oracle)
    
    æ ¸å¿ƒä¼˜åŒ–ï¼š
    1. å¼‚æ­¥å¹¶å‘è¯·æ±‚ï¼ˆasyncio.gatherï¼‰
    2. åŒè§†å›¾æ•°æ®ç»“æ„ï¼ˆstable + liveï¼‰
    3. æ—¶é—´å¯¹é½éªŒè¯
    """
    
    def __init__(self, client: BinanceClient = None):
        """
        åˆå§‹åŒ–æ•°æ®åŒæ­¥å®˜
        
        Args:
            client: Binanceå®¢æˆ·ç«¯å®ä¾‹ï¼Œå¦‚æœä¸ºNoneåˆ™è‡ªåŠ¨åˆ›å»º
        """
        self.client = client or BinanceClient()
        
        # WebSocket ç®¡ç†å™¨ï¼ˆå¯é€‰ï¼Œé»˜è®¤ç¦ç”¨ä»¥é¿å…äº‹ä»¶å¾ªç¯å†²çªï¼‰
        import os
        self.use_websocket = os.getenv("USE_WEBSOCKET", "false").lower() == "true"
        self.ws_managers = {}
        self._initial_load_complete = {}
        self._ws_disabled_symbols = set()
        
        if self.use_websocket:
            log.info("ğŸš€ WebSocket æ•°æ®æµå·²å¯ç”¨")
        else:
            log.info("ğŸ“¡ Using REST API mode (WebSocket disabled)")
        
        self.last_snapshot = None
        
        # Initialize K-line cache for incremental fetching
        self._kline_cache = get_kline_cache()
        
        log.info("ğŸ•µï¸ The Oracle (DataSync Agent) initialized")
    
    async def fetch_all_timeframes(
        self,
        symbol: str = "BTCUSDT",
        limit: int = 300
    ) -> MarketSnapshot:
        """
        å¼‚æ­¥å¹¶å‘è·å–æ‰€æœ‰å‘¨æœŸæ•°æ®
        
        Args:
            symbol: äº¤æ˜“å¯¹
            limit: æ¯ä¸ªå‘¨æœŸè·å–çš„Kçº¿æ•°é‡
            
        Returns:
            MarketSnapshotå¯¹è±¡ï¼ŒåŒ…å«åŒè§†å›¾æ•°æ®
        """
        start_time = datetime.now()
        
        # log.oracle(f"ğŸ“Š å¼€å§‹å¹¶å‘è·å– {symbol} æ•°æ®...")
        
        use_rest_fallback = False
        symbol_key = symbol.upper()
        ws_manager = None
        ws_enabled = self.use_websocket and symbol_key not in self._ws_disabled_symbols
        
        # WebSocket æ¨¡å¼ï¼šä»ç¼“å­˜è·å–æ•°æ®
        if ws_enabled:
            ws_manager = self.ws_managers.get(symbol_key)
            if not ws_manager:
                try:
                    from src.api.binance_websocket import BinanceWebSocketManager
                    ws_manager = BinanceWebSocketManager(
                        symbol=symbol_key,
                        timeframes=['5m', '15m', '1h']
                    )
                    ws_manager.start()
                    self.ws_managers[symbol_key] = ws_manager
                    log.info(f"ğŸš€ WebSocket Manager started: {symbol_key}")
                except RuntimeError as e:
                    if "event loop" in str(e).lower():
                        log.warning(f"[{symbol}] WebSocket äº‹ä»¶å¾ªç¯å†²çªï¼Œå›é€€åˆ° REST API: {e}")
                    else:
                        log.warning(f"[{symbol}] WebSocket å¯åŠ¨å¤±è´¥ (RuntimeError)ï¼Œå›é€€åˆ° REST API: {e}")
                    self._ws_disabled_symbols.add(symbol_key)
                    ws_enabled = False
                except Exception as e:
                    log.warning(f"[{symbol}] WebSocket å¯åŠ¨å¤±è´¥ï¼Œå›é€€åˆ° REST API: {e}")
                    self._ws_disabled_symbols.add(symbol_key)
                    ws_enabled = False

        if ws_enabled and ws_manager and self._initial_load_complete.get(symbol_key):
            # ä» WebSocket ç¼“å­˜è·å–æ•°æ®
            k5m = ws_manager.get_klines('5m', limit)
            k15m = ws_manager.get_klines('15m', limit)
            k1h = ws_manager.get_klines('1h', limit)
            
            # æ£€æŸ¥æ•°æ®æ˜¯å¦è¶³å¤Ÿ
            min_len = min(len(k5m), len(k15m), len(k1h))
            if min_len < limit:
                log.warning(f"[{symbol}] WebSocket ç¼“å­˜æ•°æ®ä¸è¶³ (min={min_len}, limit={limit})ï¼Œå›é€€åˆ° REST API")
                use_rest_fallback = True
            else:
                # ä»éœ€å¼‚æ­¥è·å–å¤–éƒ¨æ•°æ®
                q_data = await quant_client.fetch_coin_data(symbol)
                # [DISABLE OI] Commented out due to API errors
                # b_funding, b_oi = await asyncio.gather(
                #     loop.run_in_executor(None, self.client.get_funding_rate_with_cache, symbol),
                #     loop.run_in_executor(None, self.client.get_open_interest, symbol)
                # )
                b_funding = await self.client.get_funding_rate_with_cache(symbol) # Run non-concurrently or just wait
                b_oi = {} # Mock empty OI

        if not ws_enabled or not self._initial_load_complete.get(symbol_key) or use_rest_fallback:
            # Get event loop for concurrent operations
            loop = asyncio.get_event_loop()
            
            # Fetch with incremental caching
            k5m = await self._fetch_with_cache(symbol_key, '5m', limit)
            k15m = await self._fetch_with_cache(symbol_key, '15m', limit)
            k1h = await self._fetch_with_cache(symbol_key, '1h', limit)
            
            # Fetch external data concurrently
            q_data = await quant_client.fetch_coin_data(symbol)
            b_funding = await loop.run_in_executor(
                None,
                self.client.get_funding_rate_with_cache,
                symbol
            )
            b_oi = {}  # Mock empty OI
            
            log.info(f"[{symbol}] Data fetched: 5m={len(k5m)}, 15m={len(k15m)}, 1h={len(k1h)}")
            
            # æ ‡è®°é¦–æ¬¡åŠ è½½å®Œæˆ
            if ws_enabled and not self._initial_load_complete.get(symbol_key):
                self._initial_load_complete[symbol_key] = True
                log.info(f"âœ… Initial data loaded ({symbol_key}), will use WebSocket cache for updates")
        
        fetch_duration = (datetime.now() - start_time).total_seconds()
        # log.oracle(f"âœ… æ•°æ®è·å–å®Œæˆï¼Œè€—æ—¶: {fetch_duration:.2f}ç§’")
        
        # æ‹†åˆ†åŒè§†å›¾
        stable_5m, live_5m = self._split_klines(k5m)
        stable_15m, live_15m = self._split_klines(k15m)
        stable_1h, live_1h = self._split_klines(k1h)

        snapshot = MarketSnapshot(
            # äº¤æ˜“å¯¹æ ‡è¯†
            symbol=symbol,  # ğŸ”§ FIX: Propagate symbol through pipeline
            # 5m æ•°æ®
            stable_5m=stable_5m,
            live_5m=live_5m,
            
            # 15m æ•°æ®
            stable_15m=stable_15m,
            live_15m=live_15m,
            
            # 1h æ•°æ®
            stable_1h=stable_1h,
            live_1h=live_1h,
            
            # å…ƒæ•°æ®
            timestamp=datetime.now(),
            alignment_ok=self._check_alignment(k5m, k15m, k1h),
            fetch_duration=fetch_duration,
            
            # åŸå§‹æ•°æ®
            raw_5m=k5m,
            raw_15m=k15m,
            raw_1h=k1h,
            quant_data=q_data,
            binance_funding=b_funding,
            binance_oi=b_oi
        )
        
        # ğŸ”® è®°å½• OI åˆ°å†å²è¿½è¸ªå™¨
        if b_oi and b_oi.get('open_interest', 0) > 0:
            oi_tracker.record(
                symbol=symbol,
                oi_value=b_oi['open_interest'],
                timestamp=b_oi.get('timestamp')
            )
        
        # ç¼“å­˜æœ€æ–°å¿«ç…§
        self.last_snapshot = snapshot
        
        # æ—¥å¿—è®°å½•
        # self._log_snapshot_info(snapshot)
        
        return snapshot
    
    async def _fetch_with_cache(self, symbol: str, interval: str, limit: int) -> List[Dict]:
        """
        Fetch K-line data with incremental caching
        
        1. Check cache for existing data
        2. If cache sufficient, fetch only new data since last timestamp
        3. Append new data to cache and return combined result
        
        Args:
            symbol: Trading pair (e.g., 'BTCUSDT')
            interval: Timeframe ('5m', '15m', '1h')
            limit: Minimum number of K-lines needed
            
        Returns:
            List of K-line dicts
        """
        loop = asyncio.get_event_loop()
        
        # Check cache
        last_ts = self._kline_cache.get_last_timestamp(symbol, interval)
        cached_df = self._kline_cache.get_cached_data(symbol, interval)
        
        if cached_df is not None and len(cached_df) >= limit and last_ts:
            # Cache sufficient - fetch only new data
            interval_ms = {
                '1m': 60 * 1000,
                '5m': 5 * 60 * 1000,
                '15m': 15 * 60 * 1000,
                '1h': 60 * 60 * 1000,
            }.get(interval, 5 * 60 * 1000)
            
            start_time = last_ts + interval_ms
            
            # Fetch only new K-lines
            new_klines = await loop.run_in_executor(
                None,
                lambda: self.client.get_klines(symbol, interval, 50, start_time=start_time)
            )
            
            if new_klines:
                # Append to cache
                self._kline_cache.append_data(symbol, interval, new_klines)
                log.debug(f"ğŸ“¦ Cache hit: {symbol}/{interval} | +{len(new_klines)} new")
            
            # Return from updated cache
            final_df = self._kline_cache.get_cached_data(symbol, interval)
            if final_df is not None and not final_df.empty:
                # Convert back to list of dicts for compatibility
                return final_df.tail(limit).to_dict('records')
            
        # Cache miss or insufficient - full fetch
        klines = await loop.run_in_executor(
            None,
            lambda: self.client.get_klines(symbol, interval, limit)
        )
        
        if klines:
            self._kline_cache.append_data(symbol, interval, klines)
            log.debug(f"ğŸ“¦ Cache miss: {symbol}/{interval} | Fetched {len(klines)} rows")
        
        return klines
    def _to_dataframe(self, klines: List[Dict]) -> pd.DataFrame:
        """
        å°†Kçº¿åˆ—è¡¨è½¬æ¢ä¸ºDataFrame
        
        Args:
            klines: Kçº¿åŸå§‹æ•°æ®åˆ—è¡¨
            
        Returns:
            å¤„ç†åçš„DataFrame
        """
        if not klines:
            return pd.DataFrame()
        
        df = pd.DataFrame(klines)
        
        # è½¬æ¢æ—¶é—´æˆ³
        if 'timestamp' in df.columns:
            df['timestamp'] = pd.to_datetime(df['timestamp'], unit='ms')
            df.set_index('timestamp', inplace=True)
        
        # ç¡®ä¿æ•°å€¼ç±»å‹
        numeric_cols = ['open', 'high', 'low', 'close', 'volume']
        for col in numeric_cols:
            if col in df.columns:
                df[col] = pd.to_numeric(df[col], errors='coerce')
        
        return df

    def _split_klines(self, klines: List[Dict]) -> Tuple[pd.DataFrame, Dict]:
        """
        Split klines into stable (closed) DataFrame and live (latest) kline dict.
        Uses is_closed when available; otherwise falls back to close_time vs now.
        """
        if not klines:
            return pd.DataFrame(), {}

        last = klines[-1]
        is_closed = last.get('is_closed')
        if is_closed is None:
            close_time = last.get('close_time')
            if close_time is not None:
                try:
                    now_ms = int(datetime.now().timestamp() * 1000)
                    is_closed = int(close_time) <= now_ms
                except (TypeError, ValueError):
                    is_closed = False
            else:
                is_closed = False

        stable_source = klines if is_closed else klines[:-1]
        return self._to_dataframe(stable_source), last
    
    def _check_alignment(
        self,
        k5m: List[Dict],
        k15m: List[Dict],
        k1h: List[Dict]
    ) -> bool:
        """
        æ£€æŸ¥å¤šå‘¨æœŸæ•°æ®çš„æ—¶é—´å¯¹é½æ€§
        
        Args:
            k5m, k15m, k1h: å„å‘¨æœŸKçº¿æ•°æ®
            
        Returns:
            True if aligned, False otherwise
        """
        if not all([k5m, k15m, k1h]):
            log.warning("âš ï¸ éƒ¨åˆ†å‘¨æœŸæ•°æ®ç¼ºå¤±ï¼Œæ—¶é—´å¯¹é½å¤±è´¥")
            return False
        
        try:
            # è·å–æœ€æ–°Kçº¿çš„æ—¶é—´æˆ³
            t5m = k5m[-1]['timestamp']
            t15m = k15m[-1]['timestamp']
            t1h = k1h[-1]['timestamp']
            
            # è®¡ç®—æ—¶é—´å·®ï¼ˆæ¯«ç§’ï¼‰
            diff_5m_15m = abs(t5m - t15m)
            diff_5m_1h = abs(t5m - t1h)
            
            # ä½¿ç”¨æ›´å®½æ¾çš„å®¹å·®:
            # - 5m vs 15m: å…è®¸ 15 åˆ†é’Ÿå·®å¼‚ (15m Kçº¿å‘¨æœŸ)
            # - 5m vs 1h: å…è®¸ 1 å°æ—¶å·®å¼‚ (1h Kçº¿å‘¨æœŸ)
            max_diff_15m = 900000   # 15 åˆ†é’Ÿ = 900,000 ms
            max_diff_1h = 3600000   # 1 å°æ—¶ = 3,600,000 ms
            
            # åªæœ‰ä¸¥é‡åå·®æ‰è­¦å‘Š
            if diff_5m_15m > max_diff_15m or diff_5m_1h > max_diff_1h:
                log.warning(
                    f"âš ï¸ æ—¶é—´å¯¹é½å¼‚å¸¸: "
                    f"5m vs 15m = {diff_5m_15m/1000:.0f}s, "
                    f"5m vs 1h = {diff_5m_1h/1000:.0f}s"
                )
                return False
            
            return True
            
        except Exception as e:
            log.error(f"âŒ æ—¶é—´å¯¹é½æ£€æŸ¥å¤±è´¥: {e}")
            return False
    
    def _log_snapshot_info(self, snapshot: MarketSnapshot):
        """è®°å½•å¿«ç…§ä¿¡æ¯"""
        log.oracle(f"ğŸ“¸ å¿«ç…§ä¿¡æ¯:")
        log.oracle(f"  - 5m:  {len(snapshot.stable_5m)} å·²å®Œæˆ + 1 å®æ—¶")
        log.oracle(f"  - 15m: {len(snapshot.stable_15m)} å·²å®Œæˆ + 1 å®æ—¶")
        log.oracle(f"  - 1h:  {len(snapshot.stable_1h)} å·²å®Œæˆ + 1 å®æ—¶")
        log.oracle(f"  - æ—¶é—´å¯¹é½: {'âœ…' if snapshot.alignment_ok else 'âŒ'}")
        log.oracle(f"  - è·å–è€—æ—¶: {snapshot.fetch_duration:.2f}ç§’")
        
        # è®°å½•å®æ—¶ä»·æ ¼
        if snapshot.live_5m:
            log.info(f"  - å®æ—¶ä»·æ ¼ (5m): ${snapshot.live_5m.get('close', 0):,.2f}")
        if snapshot.live_1h:
            log.info(f"  - å®æ—¶ä»·æ ¼ (1h): ${snapshot.live_1h.get('close', 0):,.2f}")
    
    def get_live_price(self, timeframe: str = '5m') -> float:
        """
        è·å–æŒ‡å®šå‘¨æœŸçš„å®æ—¶ä»·æ ¼
        
        Args:
            timeframe: '5m', '15m', or '1h'
            
        Returns:
            å®æ—¶æ”¶ç›˜ä»·
        """
        if not self.last_snapshot:
            log.warning("âš ï¸ æ— å¯ç”¨å¿«ç…§")
            return 0.0
        
        live_data = {
            '5m': self.last_snapshot.live_5m,
            '15m': self.last_snapshot.live_15m,
            '1h': self.last_snapshot.live_1h
        }.get(timeframe, {})
        
        return float(live_data.get('close', 0))
    
    def get_stable_dataframe(self, timeframe: str = '5m') -> pd.DataFrame:
        """
        è·å–æŒ‡å®šå‘¨æœŸçš„ç¨³å®šDataFrameï¼ˆå·²å®ŒæˆKçº¿ï¼‰
        
        Args:
            timeframe: '5m', '15m', or '1h'
            
        Returns:
            å·²å®Œæˆçš„Kçº¿DataFrame
        """
        if not self.last_snapshot:
            log.warning("âš ï¸ æ— å¯ç”¨å¿«ç…§")
            return pd.DataFrame()
        
        return {
            '5m': self.last_snapshot.stable_5m,
            '15m': self.last_snapshot.stable_15m,
            '1h': self.last_snapshot.stable_1h
        }.get(timeframe, pd.DataFrame())


# å¼‚æ­¥æµ‹è¯•å‡½æ•°
async def test_data_sync_agent():
    """æµ‹è¯•æ•°æ®åŒæ­¥å®˜"""
    agent = DataSyncAgent()
    
    print("\n" + "="*80)
    print("æµ‹è¯•ï¼šæ•°æ®åŒæ­¥å®˜ (Data Sync Agent)")
    print("="*80)
    
    # æµ‹è¯•1: å¹¶å‘è·å–æ•°æ®
    print("\n[æµ‹è¯•1] å¹¶å‘è·å–å¤šå‘¨æœŸæ•°æ®...")
    snapshot = await agent.fetch_all_timeframes("BTCUSDT")
    
    print(f"\nâœ… æ•°æ®è·å–æˆåŠŸ")
    print(f"  - è€—æ—¶: {snapshot.fetch_duration:.2f}ç§’")
    print(f"  - æ—¶é—´å¯¹é½: {snapshot.alignment_ok}")
    
    # æµ‹è¯•2: éªŒè¯åŒè§†å›¾
    print("\n[æµ‹è¯•2] éªŒè¯åŒè§†å›¾æ•°æ®...")
    print(f"  - Stable 5m shape: {snapshot.stable_5m.shape}")
    print(f"  - Live 5m keys: {list(snapshot.live_5m.keys())}")
    print(f"  - Live 5m price: ${snapshot.live_5m.get('close', 0):,.2f}")
    
    # æµ‹è¯•3: è·å–å®æ—¶ä»·æ ¼
    print("\n[æµ‹è¯•3] è·å–å®æ—¶ä»·æ ¼...")
    for tf in ['5m', '15m', '1h']:
        price = agent.get_live_price(tf)
        print(f"  - {tf}: ${price:,.2f}")
    
    print("\n" + "="*80)
    print("âœ… æ‰€æœ‰æµ‹è¯•é€šè¿‡")
    print("="*80 + "\n")


if __name__ == "__main__":
    # è¿è¡Œæµ‹è¯•
    asyncio.run(test_data_sync_agent())
