"""
å†å²æ•°æ®å›æ”¾å™¨ (Data Replay Agent)
===================================

æ¨¡æ‹Ÿ DataSyncAgentï¼Œä»å†å²æ•°æ®ç”Ÿæˆ MarketSnapshot
ç”¨äºå›æµ‹æ—¶æä¾›ä¸å®ç›˜ç›¸åŒçš„æ•°æ®æ¥å£

Author: AI Trader Team
Date: 2025-12-31
"""

import asyncio
from datetime import datetime, timedelta, timezone
from typing import Dict, List, Iterator, Optional, Tuple
from dataclasses import dataclass, field
import pandas as pd
import numpy as np
import os

from src.api.binance_client import BinanceClient
from src.agents.data_sync_agent import MarketSnapshot
from src.utils.logger import log


@dataclass
class FundingRateRecord:
    """èµ„é‡‘è´¹ç‡è®°å½•"""
    timestamp: datetime
    funding_rate: float
    mark_price: float


@dataclass
class DataCache:
    """å†å²æ•°æ®ç¼“å­˜"""
    symbol: str
    df_5m: pd.DataFrame
    df_15m: pd.DataFrame
    df_1h: pd.DataFrame
    start_date: datetime
    end_date: datetime
    funding_rates: List['FundingRateRecord'] = field(default_factory=list)  # èµ„é‡‘è´¹ç‡å†å²



class DataReplayAgent:
    """
    å†å²æ•°æ®å›æ”¾å™¨
    
    åŠŸèƒ½ï¼š
    1. ä» Binance è·å–å†å² K çº¿æ•°æ®
    2. æœ¬åœ°ç¼“å­˜ï¼ˆParquet æ ¼å¼ï¼‰
    3. åœ¨æŒ‡å®šæ—¶é—´ç‚¹ç”Ÿæˆ MarketSnapshot
    4. æ¨¡æ‹Ÿå®æ—¶æ•°æ®æµç”¨äºå›æµ‹
    """
    
    CACHE_DIR = "data/backtest_cache"
    
    def __init__(
        self,
        symbol: str,
        start_date: str,
        end_date: str,
        client: BinanceClient = None
    ):
        """
        åˆå§‹åŒ–æ•°æ®å›æ”¾å™¨
        
        Args:
            symbol: äº¤æ˜“å¯¹ (e.g., "BTCUSDT")
            start_date: å¼€å§‹æ—¥æœŸ "YYYY-MM-DD" æˆ– "YYYY-MM-DD HH:MM"
            end_date: ç»“æŸæ—¥æœŸ "YYYY-MM-DD" æˆ– "YYYY-MM-DD HH:MM"
            client: Binance å®¢æˆ·ç«¯ï¼ˆå¯é€‰ï¼‰
        """
        self.symbol = symbol

        start_dt, _ = self._parse_input_date(start_date)
        end_dt, end_has_time = self._parse_input_date(end_date)
        if not end_has_time:
            end_dt = end_dt + timedelta(days=1)

        # Normalize to UTC-naive to match Binance UTC timestamps
        self.start_date = self._to_utc_naive(start_dt)
        self.end_date = self._to_utc_naive(end_dt)
            
        self.client = client or BinanceClient()
        
        # æ•°æ®ç¼“å­˜
        self.data_cache: Optional[DataCache] = None
        
        # å½“å‰å›æ”¾ä½ç½®
        self.current_idx = 0
        self.timestamps: List[datetime] = []
        
        # æœ€æ–°å¿«ç…§ï¼ˆæ¨¡æ‹Ÿ DataSyncAgent.latest_snapshotï¼‰
        self.latest_snapshot: Optional[MarketSnapshot] = None
        
        # ç¡®ä¿ç¼“å­˜ç›®å½•å­˜åœ¨
        os.makedirs(self.CACHE_DIR, exist_ok=True)
        
        log.info(f"ğŸ“¼ DataReplayAgent initialized | {symbol} | {self.start_date} to {self.end_date}")
    
    async def load_data(self) -> bool:
        """
        åŠ è½½å†å²æ•°æ®ï¼ˆä¼˜å…ˆä»ç¼“å­˜è¯»å–ï¼‰
        
        Returns:
            æ˜¯å¦æˆåŠŸåŠ è½½
        """
        cache_file = self._get_cache_path()
        
        # å°è¯•ä»ç¼“å­˜åŠ è½½
        if os.path.exists(cache_file):
            log.info(f"ğŸ“‚ Loading cached data from {cache_file}")
            try:
                self._load_from_cache(cache_file)
                # Verify we actually got data for range
                if not self.timestamps:
                    log.warning("Cache loaded but no timestamps in range. Retrying fetch...")
                elif not self._cache_covers_range():
                    log.warning(
                        "Cache range incomplete for requested window "
                        f"(expected 5m {self._expected_start_5m()} -> {self._expected_end_5m()}, "
                        f"cache {self._describe_cache_range()}). Refetching..."
                    )
                else:
                    log.info(f"âœ… Loaded {len(self.timestamps)} timestamps from cache")
                    return True
            except Exception as e:
                log.warning(f"Cache load failed: {e}, fetching from API...")
        
        # ä» API è·å–
        log.info(f"ğŸ“¥ Fetching historical data from Binance API...")
        try:
            await self._fetch_from_api()
            if not self._cache_covers_range():
                log.error(
                    "Fetched data does not fully cover requested range "
                    f"(expected 5m {self._expected_start_5m()} -> {self._expected_end_5m()}, "
                    f"cache {self._describe_cache_range()})."
                )
                return False
            # ä¿å­˜åˆ°ç¼“å­˜
            self._save_to_cache(cache_file)
            log.info(f"âœ… Fetched and cached {len(self.timestamps)} timestamps")
            return True
        except Exception as e:
            log.error(f"âŒ Failed to fetch historical data: {e}")
            return False

    def _parse_input_date(self, value: str) -> Tuple[datetime, bool]:
        """è§£æè¾“å…¥æ—¥æœŸï¼Œè¿”å› (datetime, æ˜¯å¦åŒ…å«æ—¶é—´)"""
        try:
            return datetime.strptime(value, "%Y-%m-%d %H:%M"), True
        except ValueError:
            return datetime.strptime(value, "%Y-%m-%d"), False

    def _to_utc_naive(self, dt: datetime) -> datetime:
        """å°†æœ¬åœ°æ—¶é—´è½¬æ¢ä¸ºUTC-naiveï¼Œé¿å…ä¸Binance UTCæ—¶é—´è½´é”™ä½"""
        if dt.tzinfo is None:
            local_tz = datetime.now().astimezone().tzinfo
            dt = dt.replace(tzinfo=local_tz)
        return dt.astimezone(timezone.utc).replace(tzinfo=None)

    def _utc_timestamp_ms(self, dt: datetime) -> int:
        """UTC-naive -> UTC æ—¶é—´æˆ³ (ms)"""
        return int(dt.replace(tzinfo=timezone.utc).timestamp() * 1000)

    def _describe_cache_range(self) -> str:
        """ç®€è¦æè¿°ç¼“å­˜æ•°æ®æ—¶é—´èŒƒå›´ï¼ˆç”¨äºè¯Šæ–­ï¼‰"""
        if self.data_cache is None:
            return "cache=None"
        def _range(df: pd.DataFrame) -> str:
            if df is None or df.empty:
                return "empty"
            return f"{df.index.min()} -> {df.index.max()}"
        return f"5m[{_range(self.data_cache.df_5m)}], 15m[{_range(self.data_cache.df_15m)}], 1h[{_range(self.data_cache.df_1h)}]"

    def _expected_start_5m(self) -> datetime:
        return pd.Timestamp(self.start_date).ceil("5min").to_pydatetime()

    def _expected_end_5m(self) -> datetime:
        end_cutoff = self.end_date - timedelta(seconds=1)
        return pd.Timestamp(end_cutoff).floor("5min").to_pydatetime()

    def _cache_covers_range(self) -> bool:
        """æ£€æŸ¥ç¼“å­˜æ˜¯å¦è¦†ç›–å®Œæ•´å›æµ‹çª—å£ï¼ˆå«å¤šå‘¨æœŸï¼‰"""
        if self.data_cache is None:
            return False
        df_5m = self.data_cache.df_5m
        df_15m = self.data_cache.df_15m
        df_1h = self.data_cache.df_1h
        if df_5m.empty or df_15m.empty or df_1h.empty:
            return False

        start_5m = pd.Timestamp(self.start_date).ceil("5min")
        start_15m = pd.Timestamp(self.start_date).ceil("15min")
        start_1h = pd.Timestamp(self.start_date).ceil("60min")

        end_cutoff = self.end_date - timedelta(seconds=1)
        end_5m = pd.Timestamp(end_cutoff).floor("5min")
        end_15m = pd.Timestamp(end_cutoff).floor("15min")
        end_1h = pd.Timestamp(end_cutoff).floor("60min")

        if df_5m.index.min() > start_5m or df_5m.index.max() < end_5m:
            return False
        if df_15m.index.min() > start_15m or df_15m.index.max() < end_15m:
            return False
        if df_1h.index.min() > start_1h or df_1h.index.max() < end_1h:
            return False

        return True
    
    def _get_cache_path(self) -> str:
        """ç”Ÿæˆç¼“å­˜æ–‡ä»¶è·¯å¾„"""
        # Use simple date string for cache key to maximize hits
        # (Even if precise time is used, we cache the whole day range usually)
        # But here start/end might be mid-day. 
        # Strategy: Cache based on DATE part only to allow reuse for different times on same days.
        start_str = self.start_date.strftime("%Y%m%d")
        
        # Use the DATE part of end_date (minus a microsecond to handle clean midnight?)
        # Actually safest is to use the requested window.
        # But if I request 16:00, and later request 00:00, different cache?
        # Ideally cache should cover the widest range.
        # For simplicity, just use the exact request strings converted to safe chars.
        end_str = self.end_date.strftime("%Y%m%d")
        
        # Include lookback in cache path to ensure invalidation when lookback changes
        lookback_days = 30  # Must match the value in _fetch_from_api
        return os.path.join(
            self.CACHE_DIR,
            f"{self.symbol}_{start_str}_{end_str}_lb{lookback_days}.parquet"
        )
    
    async def _fetch_from_api(self):
        """ä» Binance API è·å–å†å²æ•°æ®"""
        # CRITICAL FIX: Need historical data BEFORE backtest period for technical indicators
        # Add lookback period (default 30 days) before start_date
        lookback_days = 30
        extended_start = self.start_date - timedelta(days=lookback_days)
        
        # Calculate total days including lookback
        total_days = (self.end_date - extended_start).days + 1
        
        # Calculate required candles
        # 5m Kçº¿ï¼šæ¯å¤© 288 æ ¹
        limit_5m = total_days * 288 * 2 # Safety factor
        # 15m Kçº¿ï¼šæ¯å¤© 96 æ ¹
        limit_15m = total_days * 96 * 2
        # 1h Kçº¿ï¼šæ¯å¤© 24 æ ¹
        limit_1h = total_days * 24 * 2
        
        log.info(f"ğŸ“Š Fetching data from {extended_start.date()} to {self.end_date.date()}")
        log.info(f"   Lookback: {lookback_days} days before backtest start")
        
        # Binance API é™åˆ¶å•æ¬¡æœ€å¤š 1500 æ ¹ï¼Œéœ€è¦åˆ†æ‰¹è·å–
        df_5m = await self._fetch_klines_batched("5m", limit_5m)
        df_15m = await self._fetch_klines_batched("15m", limit_15m)
        df_1h = await self._fetch_klines_batched("1h", limit_1h)
        
        # è·å–èµ„é‡‘è´¹ç‡å†å²
        funding_rates = await self._fetch_funding_rates()
        
        # IMPORTANT: Do NOT filter out historical data before start_date here
        # We need it for technical indicator calculation
        # Only filter data AFTER end_date
        df_5m = df_5m[df_5m.index <= self.end_date]
        df_15m = df_15m[df_15m.index <= self.end_date]
        df_1h = df_1h[df_1h.index <= self.end_date]
        
        # åˆ›å»ºç¼“å­˜å¯¹è±¡
        self.data_cache = DataCache(
            symbol=self.symbol,
            df_5m=df_5m,
            df_15m=df_15m,
            df_1h=df_1h,
            start_date=self.start_date,
            end_date=self.end_date,
            funding_rates=funding_rates
        )
        
        # Generate timestamp list (based on 5m K-lines)
        all_timestamps = df_5m.index.tolist()
        
        # Filter timestamps to backtest period only
        # Strict inequality for end_date to avoid processing the exact end second if not in data
        self.timestamps = [ts for ts in all_timestamps if self.start_date <= ts < self.end_date]
        
        log.info(f"   Backtest timestamps (5m): {len(self.timestamps)}")
        if self.timestamps:
            log.info(f"   First: {self.timestamps[0]}, Last: {self.timestamps[-1]}")
    
    async def _fetch_funding_rates(self) -> List[FundingRateRecord]:
        """è·å–èµ„é‡‘è´¹ç‡å†å²æ•°æ®"""
        funding_records = []
        
        try:
            # è®¡ç®—æ—¶é—´èŒƒå›´
            start_ts = self._utc_timestamp_ms(self.start_date)
            end_ts = self._utc_timestamp_ms(self.end_date)
            
            # Binance API æ¯æ¬¡æœ€å¤šè¿”å› 1000 æ¡
            current_start = start_ts
            
            # Safety break
            loop_count = 0
            while current_start < end_ts and loop_count < 100:
                loop_count += 1
                funding_data = self.client.client.futures_funding_rate(
                    symbol=self.symbol,
                    startTime=current_start,
                    endTime=end_ts,
                    limit=1000
                )
                
                if not funding_data:
                    break
                
                for record in funding_data:
                    fr = FundingRateRecord(
                        timestamp=datetime.fromtimestamp(record['fundingTime'] / 1000),
                        funding_rate=float(record['fundingRate']),
                        mark_price=float(record.get('markPrice', 0))
                    )
                    funding_records.append(fr)
                
                if len(funding_data) < 1000:
                    break
                
                # ä¸‹ä¸€æ‰¹ä»æœ€åä¸€æ¡æ—¶é—´ +1 å¼€å§‹
                current_start = funding_data[-1]['fundingTime'] + 1
                await asyncio.sleep(0.1)  # é¿å…è¯·æ±‚è¿‡å¿«
            
            log.info(f"ğŸ“Š Fetched {len(funding_records)} funding rate records")
            
        except Exception as e:
            log.warning(f"âš ï¸ Failed to fetch funding rates: {e}")
        
        return funding_records
    
    async def _fetch_klines_batched(self, interval: str, total_limit: int) -> pd.DataFrame:
        """åˆ†æ‰¹è·å– K çº¿æ•°æ®"""
        all_klines = []
        batch_size = 1000  # Binance æ¨èçš„æ‰¹æ¬¡å¤§å°
        
        # è®¡ç®—ç»“æŸæ—¶é—´æˆ³
        end_ts = self._utc_timestamp_ms(self.end_date)
        
        remaining = total_limit
        current_end = end_ts
        loop_count = 0
        
        while remaining > 0 and loop_count < 200: # Safety break
            loop_count += 1
            limit = min(batch_size, remaining)
            
            try:
                klines = self.client.client.futures_klines(
                    symbol=self.symbol,
                    interval=interval,
                    endTime=current_end,
                    limit=limit
                )
                
                if not klines:
                    break
                
                all_klines = klines + all_klines  # å€’åºæ’å…¥
                
                # æ›´æ–°ä¸‹ä¸€æ‰¹çš„ç»“æŸæ—¶é—´ï¼ˆå–æœ€æ—©ä¸€æ ¹çš„å¼€å§‹æ—¶é—´ - 1ï¼‰
                current_end = klines[0][0] - 1
                remaining -= len(klines)
                
                # é¿å…è¯·æ±‚è¿‡å¿«
                await asyncio.sleep(0.1)
                
            except Exception as e:
                log.warning(f"Batch fetch error: {e}")
                break
        
        # è½¬æ¢ä¸º DataFrame
        return self._klines_to_dataframe(all_klines)
    
    def _klines_to_dataframe(self, klines: List) -> pd.DataFrame:
        """å°† K çº¿åˆ—è¡¨è½¬æ¢ä¸º DataFrame"""
        if not klines:
            return pd.DataFrame()
        
        df = pd.DataFrame(klines, columns=[
            'timestamp', 'open', 'high', 'low', 'close', 'volume',
            'close_time', 'quote_volume', 'trades', 'taker_buy_base',
            'taker_buy_quote', 'ignore'
        ])
        
        # è½¬æ¢æ•°æ®ç±»å‹
        df['timestamp'] = pd.to_datetime(df['timestamp'], unit='ms')
        df.set_index('timestamp', inplace=True)
        
        for col in ['open', 'high', 'low', 'close', 'volume', 'quote_volume']:
            df[col] = df[col].astype(float)
        
        df['trades'] = df['trades'].astype(int)
        
        return df[['open', 'high', 'low', 'close', 'volume', 'quote_volume', 'trades']]
    
    def _filter_date_range(self, df: pd.DataFrame) -> pd.DataFrame:
        """è¿‡æ»¤æ—¥æœŸèŒƒå›´"""
        if df.empty:
            return df
        # Use < instead of <= since end_date is now strictly parsed
        return df[(df.index >= self.start_date) & (df.index < self.end_date)]
    
    def _save_to_cache(self, cache_path: str):
        """ä¿å­˜æ•°æ®åˆ°ç¼“å­˜"""
        if self.data_cache is None:
            return
        
        # åˆå¹¶æ‰€æœ‰æ•°æ®
        cache_data = {
            'df_5m': self.data_cache.df_5m,
            'df_15m': self.data_cache.df_15m,
            'df_1h': self.data_cache.df_1h,
            'funding_rates': [
                {'timestamp': fr.timestamp, 'funding_rate': fr.funding_rate, 'mark_price': fr.mark_price}
                for fr in self.data_cache.funding_rates
            ],
            # Save date boundaries to verify cache validity
            'start_date': self.start_date,
            'end_date': self.end_date
        }
        
        # ä½¿ç”¨ pickle ä¿å­˜ï¼ˆæ”¯æŒå¤šä¸ª DataFrameï¼‰
        import pickle
        with open(cache_path, 'wb') as f:
            pickle.dump(cache_data, f)
    
    def _load_from_cache(self, cache_path: str):
        """ä»ç¼“å­˜åŠ è½½æ•°æ®"""
        import pickle
        with open(cache_path, 'rb') as f:
            cache_data = pickle.load(f)
        
        # åŠ è½½èµ„é‡‘è´¹ç‡ï¼ˆå…¼å®¹æ—§ç¼“å­˜ï¼‰
        funding_rates = []
        if 'funding_rates' in cache_data:
            for fr_dict in cache_data['funding_rates']:
                funding_rates.append(FundingRateRecord(
                    timestamp=fr_dict['timestamp'],
                    funding_rate=fr_dict['funding_rate'],
                    mark_price=fr_dict.get('mark_price', 0)
                ))
        
        # Reconstruct DataCache
        self.data_cache = DataCache(
            symbol=self.symbol,
            df_5m=cache_data['df_5m'],
            df_15m=cache_data['df_15m'],
            df_1h=cache_data['df_1h'],
            start_date=self.start_date,
            end_date=self.end_date,
            funding_rates=funding_rates
        )
        
        self.timestamps = [ts for ts in self.data_cache.df_5m.index.tolist() if self.start_date <= ts < self.end_date]
        
        log.info(f"   Date comparison: start_date={self.start_date}, end_date={self.end_date}")
        if not self.timestamps:
            log.warning(f"   âš ï¸ Cache loaded but zero timestamps in requested range!")
        else:
            log.info(f"   Cached range: {self.timestamps[0]} to {self.timestamps[-1]}")
            log.info(f"   Backtest timestamps: {len(self.timestamps)}")
    
    def get_snapshot_at(self, timestamp: datetime, lookback: int = 1000) -> MarketSnapshot:
        """
        è·å–æŒ‡å®šæ—¶é—´ç‚¹çš„å¸‚åœºå¿«ç…§
        
        Args:
            timestamp: ç›®æ ‡æ—¶é—´ç‚¹
            lookback: å›çœ‹çš„ K çº¿æ•°é‡ (5m candles). Defaults to 1000 (~3.5 days) to ensure enough 1h data.
            
        Returns:
            MarketSnapshot å¯¹è±¡ï¼ˆä¸ DataSyncAgent å…¼å®¹ï¼‰
        """
        if self.data_cache is None:
            raise ValueError("Data not loaded. Call load_data() first.")
        
        # è·å–æˆªæ­¢åˆ° timestamp çš„æ•°æ®
        # Ensure we have enough data for 1h analysis (need > 60 candles)
        # 1000 5m candles = 83 1h candles.
        
        df_5m = self.data_cache.df_5m[self.data_cache.df_5m.index <= timestamp].tail(lookback)
        
        # For 15m and 1h, we need at least 100 candles to be safe for indicators
        lb_15m = max(lookback // 3, 100)
        lb_1h = max(lookback // 12, 100)
        
        df_15m = self.data_cache.df_15m[self.data_cache.df_15m.index <= timestamp].tail(lb_15m)
        df_1h = self.data_cache.df_1h[self.data_cache.df_1h.index <= timestamp].tail(lb_1h)
        
        # Stable view: æ’é™¤æœ€åä¸€æ ¹ï¼ˆæœªå®Œæˆï¼‰
        # Live view: æœ€åä¸€æ ¹ï¼ˆä½œä¸º Dictï¼‰
        live_5m_dict = df_5m.iloc[-1].to_dict() if len(df_5m) > 0 else {}
        live_15m_dict = df_15m.iloc[-1].to_dict() if len(df_15m) > 0 else {}
        live_1h_dict = df_1h.iloc[-1].to_dict() if len(df_1h) > 0 else {}
        
        funding_snapshot = {}
        fr_record = self.get_funding_rate_at(timestamp)
        if fr_record:
            funding_snapshot = {
                'funding_rate': fr_record.funding_rate,
                'timestamp': fr_record.timestamp,
                'mark_price': fr_record.mark_price
            }

        snapshot = MarketSnapshot(
            stable_5m=df_5m.iloc[:-1] if len(df_5m) > 1 else df_5m,
            stable_15m=df_15m.iloc[:-1] if len(df_15m) > 1 else df_15m,
            stable_1h=df_1h.iloc[:-1] if len(df_1h) > 1 else df_1h,
            live_5m=live_5m_dict,
            live_15m=live_15m_dict,
            live_1h=live_1h_dict,
            timestamp=timestamp,
            alignment_ok=self._check_alignment(df_5m, df_15m, df_1h),
            fetch_duration=0.0,
            binance_funding=funding_snapshot,
            binance_oi={}
        )
        
        self.latest_snapshot = snapshot
        return snapshot
    
    def iterate_timestamps(self, step: int = 1) -> Iterator[datetime]:
        """
        è¿­ä»£æ‰€æœ‰å›æµ‹æ—¶é—´ç‚¹
        
        Args:
            step: æ­¥é•¿ï¼ˆ1 = æ¯ 5 åˆ†é’Ÿï¼Œ3 = æ¯ 15 åˆ†é’Ÿï¼Œ12 = æ¯å°æ—¶ï¼‰
            
        Yields:
            datetime æ—¶é—´ç‚¹
        """
        for i in range(0, len(self.timestamps), step):
            self.current_idx = i
            yield self.timestamps[i]
    
    def get_current_price(self) -> float:
        """
        è·å–å½“å‰ä»·æ ¼
        
        CRITICAL FIX (Cycle 2):
        é˜²æ­¢ Look-ahead Biasï¼š
        è¿”å›å½“å‰ K çº¿çš„ Open ä»·æ ¼ï¼Œè€Œä¸æ˜¯ Close ä»·æ ¼ã€‚
        åœ¨å›æµ‹æ—¶åˆ» Tï¼Œæˆ‘ä»¬åªèƒ½çœ‹åˆ° T æ—¶åˆ»çš„å¼€ç›˜ä»·ï¼Œçœ‹ä¸åˆ° T+5m çš„æ”¶ç›˜ä»·ã€‚
        """
        if self.latest_snapshot is None:
            return 0.0
        
        live = self.latest_snapshot.live_5m
        if isinstance(live, dict):
            # ä½¿ç”¨ OPEN ä»·æ ¼
            return float(live.get('open', 0.0))
        elif hasattr(live, 'empty') and not live.empty:
            # ä½¿ç”¨ OPEN ä»·æ ¼
            return float(live['open'].iloc[-1])
        return 0.0
    
    def get_open_price(self) -> float:
        """
        è·å–å½“å‰ K çº¿çš„å¼€ç›˜ä»·
        
        ç”¨äºé˜²æ­¢ Look-ahead Biasï¼š
        - ä¿¡å·è®¡ç®—ä½¿ç”¨ bar[i-1] çš„æ•°æ®
        - äº¤æ˜“æ‰§è¡Œä½¿ç”¨ bar[i] çš„å¼€ç›˜ä»·
        """
        if self.latest_snapshot is None:
            return 0.0
        
        live = self.latest_snapshot.live_5m
        if isinstance(live, dict):
            return float(live.get('open', 0.0))
        elif hasattr(live, 'empty') and not live.empty:
            return float(live['open'].iloc[-1])
        return 0.0
    
    def get_previous_close_price(self) -> float:
        """
        è·å–ä¸Šä¸€æ ¹ K çº¿çš„æ”¶ç›˜ä»·
        
        ç”¨äº Look-ahead Bias é˜²æŠ¤çš„ä¿¡å·è®¡ç®—
        """
        if self.latest_snapshot is None:
            return 0.0
        
        stable = self.latest_snapshot.stable_5m
        if hasattr(stable, 'empty') and not stable.empty:
            return float(stable['close'].iloc[-1])
        return self.get_open_price()
    
    def get_progress(self) -> Tuple[int, int, float]:
        """è·å–å›æ”¾è¿›åº¦"""
        total = len(self.timestamps)
        current = self.current_idx
        pct = (current / total * 100) if total > 0 else 0
        return current, total, pct
    
    def get_funding_rate_at(self, timestamp: datetime) -> Optional[FundingRateRecord]:
        """
        è·å–æŒ‡å®šæ—¶é—´ç‚¹æˆ–ä¹‹å‰æœ€è¿‘çš„èµ„é‡‘è´¹ç‡
        
        Binance èµ„é‡‘è´¹ç‡æ¯ 8 å°æ—¶ç»“ç®—ï¼ˆUTC 00:00, 08:00, 16:00ï¼‰
        """
        if self.data_cache is None or not self.data_cache.funding_rates:
            return None
        
        # æ‰¾åˆ°æ—¶é—´æˆ³ä¹‹å‰æœ€è¿‘çš„èµ„é‡‘è´¹ç‡
        latest_fr = None
        for fr in self.data_cache.funding_rates:
            if fr.timestamp <= timestamp:
                latest_fr = fr
            else:
                break
        
        return latest_fr
    
    def is_funding_settlement_time(self, timestamp: datetime) -> bool:
        """
        æ£€æŸ¥æ˜¯å¦æ˜¯èµ„é‡‘è´¹ç‡ç»“ç®—æ—¶é—´
        
        Binance åˆçº¦èµ„é‡‘è´¹ç‡ç»“ç®—æ—¶é—´ï¼šUTC 00:00, 08:00, 16:00
        """
        if timestamp.tzinfo is not None:
            ts_utc = timestamp.astimezone(timezone.utc)
        else:
            ts_utc = timestamp
        utc_hour = ts_utc.hour
        utc_minute = ts_utc.minute
        
        # æ£€æŸ¥æ˜¯å¦ä¸ºç»“ç®—æ—¶åˆ»ï¼ˆå…è®¸å‡ åˆ†é’Ÿè¯¯å·®ï¼‰
        if utc_hour in [0, 8, 16] and utc_minute < 10:
            return True
        return False

    def _check_alignment(
        self,
        df_5m: pd.DataFrame,
        df_15m: pd.DataFrame,
        df_1h: pd.DataFrame
    ) -> bool:
        """æ£€æŸ¥å¤šå‘¨æœŸæ•°æ®å¯¹é½æ€§ï¼ˆåŸºäºç´¢å¼•æ—¶é—´æˆ³ï¼‰"""
        if df_5m.empty or df_15m.empty or df_1h.empty:
            return False

        try:
            t5m = df_5m.index[-1]
            t15m = df_15m.index[-1]
            t1h = df_1h.index[-1]

            diff_5m_15m = abs((t5m - t15m).total_seconds())
            diff_5m_1h = abs((t5m - t1h).total_seconds())

            max_diff_15m = 15 * 60
            max_diff_1h = 60 * 60

            return diff_5m_15m <= max_diff_15m and diff_5m_1h <= max_diff_1h
        except Exception:
            return False
    
    def get_funding_rate_for_settlement(self, timestamp: datetime) -> Optional[float]:
        """
        è·å–ç»“ç®—æ—¶åˆ»é€‚ç”¨çš„èµ„é‡‘è´¹ç‡ï¼ˆä»…åœ¨ç»“ç®—æ—¶åˆ»è¿”å›ï¼Œå¦åˆ™è¿”å›Noneï¼‰
        """
        if not self.is_funding_settlement_time(timestamp):
            return None
        
        fr = self.get_funding_rate_at(timestamp)
        if fr and abs((fr.timestamp - timestamp).total_seconds()) < 600:  # 10åˆ†é’Ÿå†…
            return fr.funding_rate
        return None
    
    # ========== DataSyncAgent å…¼å®¹æ¥å£ ==========
    
    async def fetch_all_timeframes(self, symbol: str = None, limit: int = 300) -> MarketSnapshot:
        """
        å…¼å®¹ DataSyncAgent.fetch_all_timeframes æ¥å£
        
        åœ¨å›æµ‹æ¨¡å¼ä¸‹ï¼Œè¿”å›å½“å‰æ—¶é—´ç‚¹çš„å¿«ç…§
        """
        if self.current_idx < len(self.timestamps):
            timestamp = self.timestamps[self.current_idx]
            return self.get_snapshot_at(timestamp, lookback=limit)
        else:
            raise IndexError("Replay finished, no more data")
    
    def get_live_price(self, timeframe: str = '5m') -> float:
        """å…¼å®¹ DataSyncAgent.get_live_price æ¥å£"""
        return self.get_current_price()
    
    def get_stable_dataframe(self, timeframe: str = '5m') -> pd.DataFrame:
        """å…¼å®¹ DataSyncAgent.get_stable_dataframe æ¥å£"""
        if self.latest_snapshot is None:
            return pd.DataFrame()
        
        if timeframe == '5m':
            return self.latest_snapshot.stable_5m
        elif timeframe == '15m':
            return self.latest_snapshot.stable_15m
        elif timeframe == '1h':
            return self.latest_snapshot.stable_1h
        else:
            return self.latest_snapshot.stable_5m


# æµ‹è¯•å‡½æ•°
async def test_data_replay():
    """æµ‹è¯•æ•°æ®å›æ”¾å™¨"""
    print("\n" + "=" * 60)
    print("ğŸ§ª Testing DataReplayAgent")
    print("=" * 60)
    
    # åˆ›å»ºå›æ”¾å™¨ï¼ˆæµ‹è¯• 7 å¤©æ•°æ®ï¼‰
    replay = DataReplayAgent(
        symbol="BTCUSDT",
        start_date="2024-12-01",
        end_date="2024-12-07"
    )
    
    # åŠ è½½æ•°æ®
    success = await replay.load_data()
    print(f"\nâœ… Data loaded: {success}")
    
    if success:
        # è¿­ä»£å‰ 5 ä¸ªæ—¶é—´ç‚¹
        print("\nğŸ“Š First 5 timestamps:")
        for i, ts in enumerate(replay.iterate_timestamps()):
            if i >= 5:
                break
            snapshot = replay.get_snapshot_at(ts)
            price = replay.get_current_price()
            print(f"   {i+1}. {ts} | Price: ${price:.2f}")
        
        # æ˜¾ç¤ºè¿›åº¦
        current, total, pct = replay.get_progress()
        print(f"\nğŸ“ˆ Progress: {current}/{total} ({pct:.1f}%)")
    
    print("\nâœ… DataReplayAgent test complete!")


if __name__ == "__main__":
    asyncio.run(test_data_replay())
