"""
LLM ç­–ç•¥æ¨ç†å¼•æ“ (Multi-Provider Support)
=========================================

æ”¯æŒå¤šç§ LLM æä¾›å•†: OpenAI, DeepSeek, Claude, Qwen, Gemini
"""
import json
import re
from typing import Dict, Optional
from src.config import config
from src.utils.logger import log
from src.strategy.llm_parser import LLMOutputParser
from src.strategy.decision_validator import DecisionValidator
from src.llm import create_client, LLMConfig


def _extract_json_robust(text: str) -> Optional[Dict]:
    """
    Robustly extract JSON from LLM response text.
    
    Tries multiple patterns to handle various LLM output formats:
    1. Markdown code block with ```json
    2. Raw JSON object
    3. Nested JSON with proper brace matching
    """
    if not text:
        return None
    
    # Pattern 1: Markdown code block
    md_match = re.search(r'```(?:json)?\s*([\s\S]*?)\s*```', text)
    if md_match:
        try:
            return json.loads(md_match.group(1))
        except json.JSONDecodeError:
            pass
    
    # Pattern 2: Find balanced JSON object
    # Find first { and match to its closing }
    start = text.find('{')
    if start != -1:
        depth = 0
        for i, char in enumerate(text[start:], start):
            if char == '{':
                depth += 1
            elif char == '}':
                depth -= 1
                if depth == 0:
                    try:
                        return json.loads(text[start:i+1])
                    except json.JSONDecodeError:
                        break
    
    return None


class StrategyEngine:
    """å¤š LLM æä¾›å•†ç­–ç•¥å†³ç­–å¼•æ“"""
    
    def __init__(self):
        # è·å– LLM é…ç½®
        llm_config = config.llm
        provider = llm_config.get('provider', 'deepseek')
        
        # è·å–å¯¹åº”æä¾›å•†çš„ API Key
        api_keys = llm_config.get('api_keys', {})
        api_key = api_keys.get(provider)
        
        # å‘åå…¼å®¹: å¦‚æœæ²¡æœ‰æ–°é…ç½®ï¼Œä½¿ç”¨æ—§çš„ deepseek é…ç½®
        if not api_key and provider == 'deepseek':
            api_key = config.deepseek.get('api_key')
        
        # LLM å‚æ•°
        self.provider = provider
        self.model = llm_config.get('model') or config.deepseek.get('model', 'deepseek-chat')
        self.temperature = llm_config.get('temperature', config.deepseek.get('temperature', 0.3))
        self.max_tokens = llm_config.get('max_tokens', config.deepseek.get('max_tokens', 2000))
        
        # åˆå§‹åŒ–è§£æå™¨å’ŒéªŒè¯å™¨
        self.parser = LLMOutputParser()
        self.validator = DecisionValidator({
            'max_leverage': config.risk.get('max_leverage', 5),
            'max_position_pct': config.risk.get('max_total_position_pct', 30.0),
            'min_risk_reward_ratio': 2.0
        })

        self.client = None
        self.is_ready = False
        
        if api_key:
            self._init_client(api_key, llm_config)
            
    def _init_client(self, api_key: str, llm_config: Dict):
        """Initialize LLM Client"""
        llm_cfg = LLMConfig(
            api_key=api_key,
            base_url=llm_config.get('base_url'),
            model=self.model,
            timeout=llm_config.get('timeout', 120),
            max_retries=llm_config.get('max_retries', 3),
            temperature=self.temperature,
            max_tokens=self.max_tokens
        )
        try:
            self.client = create_client(self.provider, llm_cfg)
            self.is_ready = True
            log.info(f"ğŸ¤– Strategy Engine initialized (Provider: {self.provider}, Model: {self.model})")
        except Exception as e:
            log.error(f"Failed to create LLM client: {e}")
            self.is_ready = False
    
    def reload_config(self):
        """Reload configuration from global config"""
        # Re-fetch config
        llm_config = config.llm
        provider = llm_config.get('provider', 'deepseek')
        api_keys = llm_config.get('api_keys', {})
        api_key = api_keys.get(provider) or config.deepseek.get('api_key')
        
        if api_key:
            self.provider = provider
            self._init_client(api_key, llm_config)
            return True
        return False
    
    def make_decision(self, market_context_text: str, market_context_data: Dict, reflection: str = None) -> Dict:
        """
        åŸºäºå¸‚åœºä¸Šä¸‹æ–‡åšå‡ºäº¤æ˜“å†³ç­–
        
        Args:
            market_context_text: æ ¼å¼åŒ–çš„å¸‚åœºä¸Šä¸‹æ–‡æ–‡æœ¬
            market_context_data: åŸå§‹å¸‚åœºæ•°æ®
            reflection: å¯é€‰çš„äº¤æ˜“åæ€æ–‡æœ¬ï¼ˆæ¥è‡ª ReflectionAgentï¼‰
            
        Returns:
            å†³ç­–ç»“æœå­—å…¸
        """
        # Ensure client is initialized
        if not self.is_ready:
            if not self.reload_config():
                log.warning("ğŸš« LLM Strategy Engine not ready (No API Key). Returning fallback.")
                return self._get_fallback_decision(market_context_data)
        
        # ğŸ‚ğŸ» Get adversarial perspectives first
        log.info("ğŸ‚ğŸ» Gathering Bull/Bear perspectives...")
        bull_perspective = self.get_bull_perspective(market_context_text)
        bear_perspective = self.get_bear_perspective(market_context_text)
        
        # ğŸ†• ä¿å­˜Bull/Bearæ—¥å¿—
        try:
            from src.server.state import global_state
            if hasattr(global_state, 'saver') and hasattr(global_state, 'current_cycle_id'):
                global_state.saver.save_bull_bear_perspectives(
                    bull=bull_perspective,
                    bear=bear_perspective,
                    symbol=market_context_data['symbol'],
                    cycle_id=global_state.current_cycle_id
                )
        except Exception as e:
            log.warning(f"Failed to save bull/bear perspectives log: {e}")
        
        system_prompt = self._build_system_prompt()
        user_prompt = self._build_user_prompt(market_context_text, bull_perspective, bear_perspective, reflection)
        
        # è®°å½• LLM è¾“å…¥
        log.llm_input(f"æ­£åœ¨å‘é€å¸‚åœºæ•°æ®åˆ° {self.provider}...", market_context_text)

        
        try:
            response = self.client.chat(
                system_prompt=system_prompt,
                user_prompt=user_prompt,
                temperature=self.temperature,
                max_tokens=self.max_tokens
            )
            
            # è·å–åŸå§‹å“åº”
            content = response.content
            
            # ä½¿ç”¨æ–°è§£æå™¨è§£æç»“æ„åŒ–è¾“å‡º
            parsed = self.parser.parse(content)
            decision = parsed['decision']
            reasoning = parsed['reasoning']
            
            # æ ‡å‡†åŒ– action å­—æ®µ
            if 'action' in decision:
                decision['action'] = self.parser.normalize_action(decision['action'])
            
            # éªŒè¯å†³ç­–
            is_valid, errors = self.validator.validate(decision)
            if not is_valid:
                log.warning(f"LLM å†³ç­–éªŒè¯å¤±è´¥: {errors}")
                log.warning(f"åŸå§‹å†³ç­–: {decision}")
                return self._get_fallback_decision(market_context_data)
            
            # è®°å½• LLM è¾“å‡º
            log.llm_output(f"{self.provider} è¿”å›å†³ç­–ç»“æœ", decision)
            if reasoning:
                log.info(f"æ¨ç†è¿‡ç¨‹:\n{reasoning}")
            
            # è®°å½•å†³ç­–
            log.llm_decision(
                action=decision.get('action', 'hold'),
                confidence=decision.get('confidence', 0),
                reasoning=decision.get('reasoning', reasoning)
            )
            
            # æ·»åŠ å…ƒæ•°æ®
            decision['timestamp'] = market_context_data['timestamp']
            decision['symbol'] = market_context_data['symbol']
            decision['model'] = self.model
            decision['raw_response'] = content
            decision['reasoning_detail'] = reasoning
            decision['validation_passed'] = True
            
            # âœ… Return full prompt for logging
            decision['system_prompt'] = system_prompt
            decision['user_prompt'] = user_prompt
            
            # ğŸ‚ğŸ» Add Bull/Bear perspectives for dashboard
            decision['bull_perspective'] = bull_perspective
            decision['bear_perspective'] = bear_perspective
            
            return decision
            
        except Exception as e:
            log.error(f"LLM decision failed: {e}")
            # è¿”å›ä¿å®ˆå†³ç­–
            return self._get_fallback_decision(market_context_data)
    
    def get_bull_perspective(self, market_context_text: str) -> Dict:
        """
        ğŸ‚ Bull Agent: Analyze market from bullish perspective
        
        Args:
            market_context_text: Formatted market context
            
        Returns:
            Dict with bullish_reasons and bull_confidence
        """
        if not self.is_ready or not self.client:
             return {"bullish_reasons": "LLM not ready", "bull_confidence": 50}

        bull_prompt = """You are a BULLISH market analyst. Your job is to find reasons WHY the market could go UP.

Analyze the provided 'Four-Layer Strategy Analysis' and identify:
1. Bullish Trend & Fuel signals (Layer 1)
2. Bullish AI Resonance (Layer 2)
3. Bullish Setup patterns (Layer 3/4)

Output your analysis in this EXACT JSON format:
```json
{
  "stance": "STRONGLY_BULLISH",
  "bullish_reasons": "Your 3-5 key bullish observations, separated by semicolons",
  "bull_confidence": 75
}
```

stance must be one of: STRONGLY_BULLISH, SLIGHTLY_BULLISH, NEUTRAL, UNCERTAIN
bull_confidence should be 0-100 based on how strong the bullish case is.
Focus ONLY on bullish factors. Ignore bearish signals."""

        try:
            response = self.client.chat(
                system_prompt=bull_prompt,
                user_prompt=market_context_text,
                temperature=0.3,
                max_tokens=500
            )
            
            content = response.content
            
            # Parse JSON from response using robust extraction
            result = _extract_json_robust(content)
            if result:
                stance = result.get('stance', 'UNKNOWN')
                log.info(f"ğŸ‚ Bull Agent: [{stance}] {result.get('bullish_reasons', '')[:40]}... (Conf: {result.get('bull_confidence', 0)}%)")
                return result
            
            return {"bullish_reasons": "Unable to analyze", "bull_confidence": 50}
            
        except Exception as e:
            log.warning(f"Bull Agent failed: {e}")
            return {"bullish_reasons": "Analysis unavailable", "bull_confidence": 50}
    
    def get_bear_perspective(self, market_context_text: str) -> Dict:
        """
        ğŸ» Bear Agent: Analyze market from bearish perspective
        
        Args:
            market_context_text: Formatted market context
            
        Returns:
            Dict with bearish_reasons and bear_confidence
        """
        if not self.is_ready or not self.client:
             return {"bearish_reasons": "LLM not ready", "bear_confidence": 50}

        bear_prompt = """You are a BEARISH market analyst. Your job is to find reasons WHY the market could go DOWN.

Analyze the provided 'Four-Layer Strategy Analysis' and identify:
1. Bearish Trend & Fuel signals (Layer 1)
2. Bearish AI Resonance (Layer 2)
3. Bearish Setup patterns (Layer 3/4)

Output your analysis in this EXACT JSON format:
```json
{
  "stance": "STRONGLY_BEARISH",
  "bearish_reasons": "Your 3-5 key bearish observations, separated by semicolons",
  "bear_confidence": 60
}
```

stance must be one of: STRONGLY_BEARISH, SLIGHTLY_BEARISH, NEUTRAL, UNCERTAIN
bear_confidence should be 0-100 based on how strong the bearish case is.
Focus ONLY on bearish factors. Ignore bullish signals."""


        try:
            response = self.client.chat(
                system_prompt=bear_prompt,
                user_prompt=market_context_text,
                temperature=0.3,
                max_tokens=500
            )
            
            content = response.content
            
            # Parse JSON from response using robust extraction
            result = _extract_json_robust(content)
            if result:
                stance = result.get('stance', 'UNKNOWN')
                log.info(f"ğŸ» Bear Agent: [{stance}] {result.get('bearish_reasons', '')[:40]}... (Conf: {result.get('bear_confidence', 0)}%)")
                return result
            
            return {"bearish_reasons": "Unable to analyze", "bear_confidence": 50}
            
        except Exception as e:
            log.warning(f"Bear Agent failed: {e}")
            return {"bearish_reasons": "Analysis unavailable", "bear_confidence": 50}
    
    def _build_system_prompt(self) -> str:
        """Build System Prompt (English Version) or Load Custom"""
        import os
        
        # Check for custom prompt
        # Assuming src/strategy/llm_engine.py, so config is ../../config/custom_prompt.md
        base_dir = os.path.dirname(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))
        custom_prompt_path = os.path.join(base_dir, 'config', 'custom_prompt.md')
        
        if os.path.exists(custom_prompt_path):
            try:
                with open(custom_prompt_path, 'r', encoding='utf-8') as f:
                    content = f.read()
                    if content.strip():
                        log.info("ğŸ“ Loading Custom System Prompt from file")
                        return content
            except Exception as e:
                log.error(f"Failed to load custom prompt: {e}")
        
        # Load default from template
        try:
            from src.config.default_prompt_template import DEFAULT_SYSTEM_PROMPT
            return DEFAULT_SYSTEM_PROMPT
        except ImportError:
            log.error("Failed to import DEFAULT_SYSTEM_PROMPT")
            return "Error: Default prompt missing"
    
    def _build_user_prompt(self, market_context: str, bull_perspective: Dict = None, bear_perspective: Dict = None, reflection: str = None) -> str:
        """Build User Prompt - DATA ONLY (No instructions, all rules are in system prompt)"""
        
        # Build adversarial analysis section (data only)
        adversarial_section = ""
        if bull_perspective or bear_perspective:
            bull_reasons = bull_perspective.get('bullish_reasons', 'N/A') if bull_perspective else 'N/A'
            bull_conf = bull_perspective.get('bull_confidence', 50) if bull_perspective else 50
            bull_stance = bull_perspective.get('stance', 'UNKNOWN') if bull_perspective else 'UNKNOWN'
            bear_reasons = bear_perspective.get('bearish_reasons', 'N/A') if bear_perspective else 'N/A'
            bear_conf = bear_perspective.get('bear_confidence', 50) if bear_perspective else 50
            bear_stance = bear_perspective.get('stance', 'UNKNOWN') if bear_perspective else 'UNKNOWN'
            
            adversarial_section = f"""
---
## ğŸ‚ğŸ» Adversarial Analysis

### ğŸ‚ Bull Agent [{bull_stance}] (Confidence: {bull_conf}%)
{bull_reasons}

### ğŸ» Bear Agent [{bear_stance}] (Confidence: {bear_conf}%)
{bear_reasons}
"""
        
        # Build reflection section (data only)
        reflection_section = ""
        if reflection:
            reflection_section = f"""
---
## ğŸ§  Trading Reflection (Last 10 Trades)

{reflection}
"""
        
        # DATA-ONLY user prompt
        return f"""# ğŸ“Š MARKET DATA INPUT

{market_context}
{adversarial_section}{reflection_section}
---

Analyze the above data following the strategy rules in system prompt. Output your decision.
"""
    
    def _get_fallback_decision(self, context: Dict) -> Dict:
        """
        è·å–å…œåº•å†³ç­–ï¼ˆå½“LLMå¤±è´¥æ—¶ï¼‰
        
        è¿”å›ä¿å®ˆçš„holdå†³ç­–
        """
        return {
            'action': 'wait',
            'symbol': context.get('symbol', 'BTCUSDT'),
            'confidence': 0,
            'leverage': 1,
            'position_size_pct': 0,
            'stop_loss_pct': 1.0,
            'take_profit_pct': 2.0,
            'reasoning': 'LLM decision failed, using conservative fallback strategy',
            'timestamp': context.get('timestamp'),
            'is_fallback': True
        }
    
    def validate_decision(self, decision: Dict) -> bool:
        """
        éªŒè¯å†³ç­–æ ¼å¼æ˜¯å¦æ­£ç¡®
        
        Returns:
            True if valid, False otherwise
        """
        required_fields = [
            'action', 'symbol', 'confidence', 'leverage',
            'position_size_pct', 'stop_loss_pct', 'take_profit_pct', 'reasoning'
        ]
        
        # æ£€æŸ¥å¿…éœ€å­—æ®µ
        for field in required_fields:
            if field not in decision:
                log.error(f"å†³ç­–ç¼ºå°‘å¿…éœ€å­—æ®µ: {field}")
                return False
        
        # æ£€æŸ¥actionåˆæ³•æ€§
        valid_actions = [
            'open_long', 'open_short', 'close_position',
            'add_position', 'reduce_position', 'hold'
        ]
        if decision['action'] not in valid_actions:
            log.error(f"æ— æ•ˆçš„action: {decision['action']}")
            return False
        
        # æ£€æŸ¥æ•°å€¼èŒƒå›´
        if not (0 <= decision['confidence'] <= 100):
            log.error(f"confidenceè¶…å‡ºèŒƒå›´: {decision['confidence']}")
            return False
        
        # STRICT ENFORCEMENT: Open trades must have confidence >= 75
        action = decision['action']
        confidence = decision['confidence']
        if action in ['open_long', 'open_short'] and confidence < 75:
            log.warning(f"ğŸš« Confidence < 75 ({confidence}%) for {action}, converting to 'wait'")
            decision['action'] = 'wait'
            decision['reasoning'] = f"Low confidence ({confidence}% < 75%), wait for better setup"
        
        if not (1 <= decision['leverage'] <= config.risk.get('max_leverage', 5)):
            log.error(f"leverageè¶…å‡ºèŒƒå›´: {decision['leverage']}")
            return False
        
        if not (0 <= decision['position_size_pct'] <= 100):
            log.error(f"position_size_pctè¶…å‡ºèŒƒå›´: {decision['position_size_pct']}")
            return False
        
        return True
