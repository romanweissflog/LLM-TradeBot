"""
å®Œæ•´äº¤æ˜“æµç¨‹æ—¥å¿—è®°å½•å™¨
è®°å½•ä»åŸå§‹æ•°æ®è·å– -> æ•°æ®å¤„ç† -> ç‰¹å¾æå– -> å¤§æ¨¡å‹å†³ç­– -> äº¤æ˜“æ‰§è¡Œçš„å…¨è¿‡ç¨‹
"""
import json
import os
from datetime import datetime
from pathlib import Path
from typing import Dict, Any, List, Optional
import pandas as pd
import numpy as np
from src.utils.json_utils import safe_json_dump


class TradingPipelineLogger:
    """äº¤æ˜“æµç¨‹å®Œæ•´æ—¥å¿—è®°å½•å™¨"""
    
    def __init__(self, log_dir: str = "logs/pipeline"):
        """åˆå§‹åŒ–æµç¨‹æ—¥å¿—è®°å½•å™¨"""
        self.log_dir = Path(log_dir)
        self.log_dir.mkdir(parents=True, exist_ok=True)
        
        # å½“å‰ä¼šè¯ä¿¡æ¯
        self.session_id = datetime.now().strftime('%Y%m%d_%H%M%S')
        self.session_start = datetime.now()
        
        # æµç¨‹è®°å½•
        self.pipeline_steps = []
        self.current_cycle = 0
        
        print(f"\n{'='*100}")
        print(f"ğŸ“Š äº¤æ˜“æµç¨‹æ—¥å¿—è®°å½•å™¨å·²å¯åŠ¨")
        print(f"ä¼šè¯ID: {self.session_id}")
        print(f"æ—¥å¿—ç›®å½•: {self.log_dir}")
        print(f"{'='*100}\n")
    
    def start_new_cycle(self, symbol: str):
        """å¼€å§‹æ–°çš„äº¤æ˜“å‘¨æœŸ"""
        self.current_cycle += 1
        self.current_cycle_data = {
            "cycle_id": self.current_cycle,
            "symbol": symbol,
            "start_time": datetime.now().isoformat(),
            "steps": []
        }
        
        print(f"\n{'='*100}")
        print(f"ğŸ”„ å¼€å§‹æ–°çš„äº¤æ˜“å‘¨æœŸ #{self.current_cycle} - {symbol}")
        print(f"â° æ—¶é—´: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
        print(f"{'='*100}\n")
    
    def log_step(self, 
                 step_name: str, 
                 step_type: str,
                 input_data: Any, 
                 processing: str, 
                 output_data: Any,
                 metadata: Optional[Dict] = None):
        """
        è®°å½•å•ä¸ªå¤„ç†æ­¥éª¤
        
        Args:
            step_name: æ­¥éª¤åç§°
            step_type: æ­¥éª¤ç±»å‹ (data_fetch|data_process|feature_extract|llm_decision|risk_check|execution)
            input_data: è¾“å…¥æ•°æ®
            processing: å¤„ç†é€»è¾‘è¯´æ˜
            output_data: è¾“å‡ºæ•°æ®
            metadata: é¢å¤–å…ƒæ•°æ®
        """
        timestamp = datetime.now().isoformat()
        
        step_log = {
            "timestamp": timestamp,
            "step_name": step_name,
            "step_type": step_type,
            "input": self._serialize_data(input_data),
            "processing": processing,
            "output": self._serialize_data(output_data),
            "metadata": metadata or {}
        }
        
        self.current_cycle_data["steps"].append(step_log)
        
        # æ‰“å°åˆ°æ§åˆ¶å°
        self._print_step(step_name, step_type, input_data, processing, output_data)
    
    def log_raw_data(self, timeframe: str, klines: List[Dict], metadata: Optional[Dict] = None):
        """è®°å½•åŸå§‹Kçº¿æ•°æ®"""
        self.log_step(
            step_name=f"1. è·å–{timeframe}åŸå§‹Kçº¿æ•°æ®",
            step_type="data_fetch",
            input_data={
                "symbol": self.current_cycle_data["symbol"],
                "interval": timeframe,
                "limit": len(klines)
            },
            processing=f"è°ƒç”¨ Binance API è·å–æœ€è¿‘{len(klines)}æ ¹{timeframe}Kçº¿",
            output_data={
                "data_type": "List[Dict]",
                "count": len(klines),
                "first_kline": klines[0] if klines else None,
                "last_kline": klines[-1] if klines else None,
                "fields": list(klines[0].keys()) if klines else []
            },
            metadata=metadata
        )
    
    def log_data_processing(self, timeframe: str, raw_count: int, df: pd.DataFrame, 
                           anomalies: int, metadata: Optional[Dict] = None):
        """è®°å½•æ•°æ®å¤„ç†æ­¥éª¤"""
        self.log_step(
            step_name=f"2. å¤„ç†{timeframe}æ•°æ®å¹¶è®¡ç®—æŠ€æœ¯æŒ‡æ ‡",
            step_type="data_process",
            input_data={
                "raw_kline_count": raw_count,
                "timeframe": timeframe
            },
            processing=f"""
æ•°æ®å¤„ç†æµç¨‹:
1. å°†Kçº¿åˆ—è¡¨è½¬ä¸ºDataFrame
2. å¼‚å¸¸å€¼æ£€æµ‹ (Z-scoreæ–¹æ³•)
3. å¼‚å¸¸å€¼æ¸…æ´— (æ£€æµ‹åˆ°{anomalies}ä¸ªå¼‚å¸¸)
4. è®¡ç®—æŠ€æœ¯æŒ‡æ ‡:
   - ç§»åŠ¨å¹³å‡: SMA(20, 50), EMA(12, 26)
   - åŠ¨é‡æŒ‡æ ‡: RSI(14), MACD(12, 26, 9)
   - æ³¢åŠ¨æŒ‡æ ‡: Bollinger Bands(20, 2), ATR(14)
   - æˆäº¤é‡æŒ‡æ ‡: Volume SMA(20), Volume Ratio
5. æ•°æ®éªŒè¯ä¸å¿«ç…§ç”Ÿæˆ
            """.strip(),
            output_data={
                "dataframe": df,
                "shape": df.shape,
                "columns": list(df.columns),
                "anomaly_count": anomalies
            },
            metadata=metadata
        )
    
    def log_feature_extraction(self, timeframe: str, features: Dict, metadata: Optional[Dict] = None):
        """è®°å½•ç‰¹å¾æå–æ­¥éª¤"""
        self.log_step(
            step_name=f"3. æå–{timeframe}å…³é”®ç‰¹å¾",
            step_type="feature_extract",
            input_data={
                "timeframe": timeframe,
                "data_source": "processed_dataframe"
            },
            processing=f"""
ä»{timeframe} DataFrameæœ€åä¸€è¡Œæå–å…³é”®ç‰¹å¾:
- ä»·æ ¼æ•°æ®: open, high, low, close
- æˆäº¤é‡: volume, volume_ratio
- è¶‹åŠ¿æŒ‡æ ‡: SMA20, SMA50, EMA12, EMA26
- åŠ¨é‡æŒ‡æ ‡: RSI, MACD, MACD_signal
- æ³¢åŠ¨æŒ‡æ ‡: Bollinger Bands (upper, middle, lower), ATR
- è®¡ç®—æŒ‡æ ‡: trend, momentum, volatility
            """.strip(),
            output_data=features,
            metadata=metadata
        )
    
    def log_multi_timeframe_context(self, context: Dict, metadata: Optional[Dict] = None):
        """è®°å½•å¤šå‘¨æœŸä¸Šä¸‹æ–‡æ„å»º"""
        self.log_step(
            step_name="4. æ„å»ºå¤šå‘¨æœŸå¸‚åœºä¸Šä¸‹æ–‡",
            step_type="feature_extract",
            input_data={
                "timeframes": list(context.keys()),
                "features_per_tf": [len(context[tf]) for tf in context.keys()]
            },
            processing="""
åˆå¹¶å¤šä¸ªæ—¶é—´å‘¨æœŸçš„ç‰¹å¾:
1. æ•´åˆå„å‘¨æœŸçš„ä»·æ ¼ã€æŒ‡æ ‡ã€è¶‹åŠ¿
2. è®¡ç®—å‘¨æœŸé—´çš„ä¸€è‡´æ€§
3. åˆ¤æ–­å¤šå‘¨æœŸè¶‹åŠ¿æ–¹å‘
4. è¯„ä¼°å¸‚åœºçŠ¶æ€ï¼ˆtrending/ranging/volatileï¼‰
            """.strip(),
            output_data=context,
            metadata=metadata
        )
    
    def log_llm_input(self, prompt_text: str, context_data: Dict, metadata: Optional[Dict] = None):
        """è®°å½•å¤§æ¨¡å‹è¾“å…¥"""
        self.log_step(
            step_name="5. å‡†å¤‡å¤§æ¨¡å‹è¾“å…¥æ•°æ®",
            step_type="llm_decision",
            input_data={
                "market_context": context_data,
                "prompt_length": len(prompt_text),
                "timeframes_analyzed": list(context_data.keys()) if isinstance(context_data, dict) else None
            },
            processing="""
æ„å»ºå¤§æ¨¡å‹è¾“å…¥:
1. å°†å¸‚åœºæ•°æ®æ ¼å¼åŒ–ä¸ºæ–‡æœ¬
2. æ·»åŠ ç³»ç»Ÿæç¤ºè¯ï¼ˆäº¤æ˜“è§„åˆ™ã€é£é™©ç®¡ç†ï¼‰
3. æ·»åŠ ç”¨æˆ·æç¤ºè¯ï¼ˆå½“å‰å¸‚åœºçŠ¶æ€ï¼‰
4. è®¾ç½®å“åº”æ ¼å¼ä¸ºJSON
            """.strip(),
            output_data={
                "prompt_preview": prompt_text[:500] + "..." if len(prompt_text) > 500 else prompt_text,
                "full_prompt_length": len(prompt_text)
            },
            metadata=metadata
        )
    
    def log_llm_output(self, decision: Dict, raw_response: str, metadata: Optional[Dict] = None):
        """è®°å½•å¤§æ¨¡å‹è¾“å‡º"""
        self.log_step(
            step_name="6. å¤§æ¨¡å‹å†³ç­–ç»“æœ",
            step_type="llm_decision",
            input_data={
                "model": decision.get('model', 'unknown'),
                "response_length": len(raw_response)
            },
            processing="""
è§£æå¤§æ¨¡å‹å“åº”:
1. æ¥æ”¶JSONæ ¼å¼çš„å†³ç­–ç»“æœ
2. éªŒè¯å†³ç­–æ ¼å¼å®Œæ•´æ€§
3. æå–å…³é”®å†³ç­–å­—æ®µ:
   - action (äº¤æ˜“åŠ¨ä½œ)
   - confidence (ç½®ä¿¡åº¦)
   - position_size_pct (ä»“ä½æ¯”ä¾‹)
   - leverage (æ æ†)
   - stop_loss_pct / take_profit_pct (æ­¢ç›ˆæ­¢æŸ)
   - reasoning (å†³ç­–ç†ç”±)
   - analysis (è¯¦ç»†åˆ†æ)
            """.strip(),
            output_data={
                "decision": decision,
                "action": decision.get('action'),
                "confidence": decision.get('confidence'),
                "reasoning": decision.get('reasoning', '')[:200]  # æˆªå–å‰200å­—ç¬¦
            },
            metadata=metadata
        )
    
    def log_risk_check(self, decision: Dict, risk_result: Dict, metadata: Optional[Dict] = None):
        """è®°å½•é£é™©æ£€æŸ¥"""
        self.log_step(
            step_name="7. é£é™©ç®¡ç†éªŒè¯",
            step_type="risk_check",
            input_data={
                "original_decision": decision.get('action'),
                "position_size_pct": decision.get('position_size_pct'),
                "leverage": decision.get('leverage')
            },
            processing="""
é£é™©ç®¡ç†æ£€æŸ¥:
1. éªŒè¯ä»“ä½å¤§å°æ˜¯å¦è¶…å‡ºé™åˆ¶
2. éªŒè¯æ æ†å€æ•°æ˜¯å¦åˆè§„
3. è®¡ç®—å®é™…é£é™©æ•å£
4. æ£€æŸ¥è´¦æˆ·ä½™é¢æ˜¯å¦è¶³å¤Ÿ
5. éªŒè¯æ­¢æŸæ­¢ç›ˆè®¾ç½®æ˜¯å¦åˆç†
6. æ£€æŸ¥æ˜¯å¦è¿åé£æ§è§„åˆ™
            """.strip(),
            output_data=risk_result,
            metadata=metadata
        )
    
    def log_execution(self, action: str, result: Dict, metadata: Optional[Dict] = None):
        """è®°å½•äº¤æ˜“æ‰§è¡Œ"""
        self.log_step(
            step_name="8. äº¤æ˜“æ‰§è¡Œ",
            step_type="execution",
            input_data={
                "action": action,
                "timestamp": datetime.now().isoformat()
            },
            processing=f"""
æ‰§è¡Œäº¤æ˜“æ“ä½œ: {action}
1. è°ƒç”¨å¸å®‰APIä¸‹å•
2. è®¾ç½®æ­¢æŸæ­¢ç›ˆè®¢å•
3. è®°å½•äº¤æ˜“æ—¥å¿—
4. æ›´æ–°æŒä»“çŠ¶æ€
            """.strip(),
            output_data=result,
            metadata=metadata
        )
    
    def end_cycle(self, final_result: Optional[Dict] = None):
        """ç»“æŸå½“å‰äº¤æ˜“å‘¨æœŸ"""
        self.current_cycle_data["end_time"] = datetime.now().isoformat()
        self.current_cycle_data["duration_seconds"] = (
            datetime.fromisoformat(self.current_cycle_data["end_time"]) - 
            datetime.fromisoformat(self.current_cycle_data["start_time"])
        ).total_seconds()
        self.current_cycle_data["final_result"] = final_result
        
        # ä¿å­˜åˆ°æµç¨‹è®°å½•
        self.pipeline_steps.append(self.current_cycle_data)
        
        # ä¿å­˜å•ä¸ªå‘¨æœŸæ—¥å¿—
        self._save_cycle_log(self.current_cycle_data)
        
        print(f"\n{'='*100}")
        print(f"âœ… äº¤æ˜“å‘¨æœŸ #{self.current_cycle} å®Œæˆ")
        print(f"â±ï¸  è€—æ—¶: {self.current_cycle_data['duration_seconds']:.2f}ç§’")
        print(f"ğŸ“Š æ€»æ­¥éª¤æ•°: {len(self.current_cycle_data['steps'])}")
        print(f"{'='*100}\n")
    
    def _serialize_data(self, data: Any) -> Any:
        """åºåˆ—åŒ–æ•°æ®ä»¥ä¾¿JSONä¿å­˜"""
        import numpy as np
        
        # å¤„ç†pandas.Timestamp
        if isinstance(data, pd.Timestamp):
            return str(data)
        
        # å¤„ç†numpyç±»å‹ï¼ˆå¿…é¡»åœ¨å…¶ä»–æ£€æŸ¥ä¹‹å‰ï¼‰
        if hasattr(data, 'item'):
            return data.item()
        
        # å¤„ç†DataFrame
        if isinstance(data, pd.DataFrame):
            df_copy = data.copy()
            df_copy.index = df_copy.index.astype(str)
            
            # è½¬æ¢æ‰€æœ‰numpyç±»å‹ä¸ºPythonåŸç”Ÿç±»å‹
            for col in df_copy.columns:
                if df_copy[col].dtype == 'object':
                    continue
                df_copy[col] = df_copy[col].apply(lambda x: x.item() if hasattr(x, 'item') else x)
            
            df_dict = df_copy.reset_index().to_dict('records')
            return {
                "type": "DataFrame",
                "shape": list(data.shape),
                "columns": list(data.columns),
                "head_3": df_dict[:3] if len(df_dict) > 0 else [],
                "tail_3": df_dict[-3:] if len(df_dict) > 0 else [],
                "summary": {
                    "row_count": len(data),
                    "column_count": len(data.columns),
                    "latest_values": self._get_latest_row_values(data)
                }
            }
        
        # å¤„ç†å­—å…¸
        elif isinstance(data, dict):
            return {str(k): self._serialize_data(v) for k, v in data.items()}
        
        # å¤„ç†åˆ—è¡¨
        elif isinstance(data, list):
            if len(data) <= 5:
                return [self._serialize_data(item) for item in data]
            else:
                return {
                    "type": "list",
                    "length": len(data),
                    "first_3": [self._serialize_data(item) for item in data[:3]],
                    "last_3": [self._serialize_data(item) for item in data[-3:]]
                }
        
        # å…¶ä»–ç±»å‹
        else:
            return data
    
    def _get_latest_row_values(self, df: pd.DataFrame) -> Dict:
        """è·å–DataFrameæœ€åä¸€è¡Œçš„å…³é”®å€¼"""
        if df.empty:
            return {}
        
        latest = df.iloc[-1]
        result = {}
        
        # åªä¿ç•™æ•°å€¼åˆ—
        for col in df.columns:
            if pd.api.types.is_numeric_dtype(df[col]):
                val = latest[col]
                if hasattr(val, 'item'):
                    result[col] = val.item()
                else:
                    result[col] = val
        
        return result
    
    def _print_step(self, step_name: str, step_type: str, input_data: Any, 
                    processing: str, output_data: Any):
        """æ‰“å°æ­¥éª¤ä¿¡æ¯åˆ°æ§åˆ¶å°"""
        
        # æ­¥éª¤ç±»å‹å›¾æ ‡
        type_icons = {
            "data_fetch": "ğŸ”½",
            "data_process": "âš™ï¸",
            "feature_extract": "ğŸ“Š",
            "llm_decision": "ğŸ¤–",
            "risk_check": "ğŸ›¡ï¸",
            "execution": "âš¡"
        }
        
        icon = type_icons.get(step_type, "ğŸ“Œ")
        
        print(f"\n{'='*100}")
        print(f"{icon} æ­¥éª¤: {step_name}")
        print(f"â° æ—¶é—´: {datetime.now().strftime('%H:%M:%S')}")
        print(f"{'='*100}")
        
        print(f"\nğŸ“¥ è¾“å…¥æ•°æ®:")
        self._print_data_summary(input_data, indent=3)
        
        print(f"\nâš™ï¸  å¤„ç†é€»è¾‘:")
        for line in processing.split('\n'):
            print(f"   {line}")
        
        print(f"\nğŸ“¤ è¾“å‡ºæ•°æ®:")
        self._print_data_summary(output_data, indent=3)
        
        print(f"\n{'='*100}")
    
    def _print_data_summary(self, data: Any, indent: int = 3):
        """æ‰“å°æ•°æ®æ‘˜è¦"""
        prefix = " " * indent
        
        if isinstance(data, pd.DataFrame):
            print(f"{prefix}ç±»å‹: DataFrame")
            print(f"{prefix}å½¢çŠ¶: {data.shape}")
            print(f"{prefix}åˆ—æ•°: {len(data.columns)}")
            
        elif isinstance(data, dict):
            print(f"{prefix}ç±»å‹: Dict")
            print(f"{prefix}é”®æ•°é‡: {len(data)}")
            for key, value in list(data.items())[:10]:  # åªæ˜¾ç¤ºå‰10ä¸ª
                if isinstance(value, (dict, list)):
                    print(f"{prefix}{key}: {type(value).__name__} (é•¿åº¦={len(value)})")
                elif isinstance(value, (int, float)):
                    if isinstance(value, float):
                        print(f"{prefix}{key}: {value:.6f}")
                    else:
                        print(f"{prefix}{key}: {value}")
                else:
                    val_str = str(value)
                    if len(val_str) > 100:
                        val_str = val_str[:100] + "..."
                    print(f"{prefix}{key}: {val_str}")
        
        elif isinstance(data, list):
            print(f"{prefix}ç±»å‹: List")
            print(f"{prefix}é•¿åº¦: {len(data)}")
        
        else:
            print(f"{prefix}{data}")
    
    def _save_cycle_log(self, cycle_data: Dict):
        """ä¿å­˜å•ä¸ªå‘¨æœŸçš„æ—¥å¿—"""
        cycle_file = self.log_dir / f"cycle_{self.session_id}_{cycle_data['cycle_id']:03d}.json"
        
        with open(cycle_file, 'w', encoding='utf-8') as f:
            safe_json_dump(cycle_data, f, indent=2, ensure_ascii=False)
        
        print(f"ğŸ’¾ å‘¨æœŸæ—¥å¿—å·²ä¿å­˜: {cycle_file}")
    
    def save_session_summary(self):
        """ä¿å­˜ä¼šè¯æ€»ç»“"""
        summary = {
            "session_id": self.session_id,
            "start_time": self.session_start.isoformat(),
            "end_time": datetime.now().isoformat(),
            "total_cycles": len(self.pipeline_steps),
            "total_duration_seconds": (datetime.now() - self.session_start).total_seconds(),
            "cycles": self.pipeline_steps
        }
        
        summary_file = self.log_dir / f"session_{self.session_id}_summary.json"
        
        with open(summary_file, 'w', encoding='utf-8') as f:
            safe_json_dump(summary, f, indent=2, ensure_ascii=False)
        
        print(f"\n{'='*100}")
        print(f"ğŸ’¾ ä¼šè¯æ€»ç»“å·²ä¿å­˜: {summary_file}")
        print(f"   æ€»å‘¨æœŸæ•°: {summary['total_cycles']}")
        print(f"   æ€»è€—æ—¶: {summary['total_duration_seconds']:.2f}ç§’")
        print(f"{'='*100}\n")
        
        return str(summary_file)
